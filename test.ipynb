{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KoSBERT 사용법\n",
    "1. [현재 리포지토리](https://github.com/KAIST-Lawgical/Sentence-Embedding-Is-All-You-Need)를 `git clone`을 통해 받는다.\n",
    "2. Lawgical KAIST의 구글 드라이브 내의 `Checkpoint` 폴더를 다운로드하여 `Sentence-Embedding-Is-All-You-Need` 바로 아래에 둔다.\n",
    "3. 디렉토리 `Sentence-Embedding-Is-All-You-Need`에서 명령어 `docker build -t sbert:1.0.0 .`를 실행하여 도커 이미지를 빌드한다.\n",
    "4. 명령어 `docker run -itd --ipc host --gpus all --name sbert sbert:1.0.0`를 도커 컨테이너를 실행한다.\n",
    "5. `Visual Studio Code`의 `remote explorer`등을 통해 도커 컨테이너에 접속한다.\n",
    "6. 도커 컨테이너 내부의 워크 디렉토리에서 이 파일, 즉 `test.ipynb`를 찾아 아래 코드를 활용하여 준비서면-판결문 문장 간 유사도 측정 작업을 실행한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import csv\n",
    "import pandas\n",
    "import pickle\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from kiwipiepy import Kiwi\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "_whitespace = re.compile(r'\\s+')\n",
    "kiwi = Kiwi()\n",
    "model_path = './Sentence-Embedding-Is-All-You-Need/Checkpoint/KoSBERT/kosbert-klue-bert-base/'\n",
    "embedder = SentenceTransformer(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collapse_whitespace(text):\n",
    "    return re.sub(_whitespace, ' ', text)\n",
    "\n",
    "\n",
    "def calculate_similarity(case_number, document_text, judgement_text, wr, top_k=1):\n",
    "    document_id = document_text[0]\n",
    "    documents = kiwi.split_into_sents(document_text[1])\n",
    "    documents = [row[0] for row in documents]\n",
    "    \n",
    "    judgement_id = judgement_text[0]\n",
    "    judgements = kiwi.split_into_sents(judgement_text[1])\n",
    "    judgements = [row[0] for row in judgements]\n",
    "    \n",
    "    if not len(documents) or not len(judgements):\n",
    "        return\n",
    "    \n",
    "    judgement_embeddings = embedder.encode(judgements, convert_to_tensor=True)\n",
    "    \n",
    "    for document in documents:\n",
    "        document_embedding = embedder.encode(document, convert_to_tensor=True)\n",
    "        cos_scores = util.pytorch_cos_sim(document_embedding, judgement_embeddings)[0]\n",
    "        cos_scores = cos_scores.cpu()\n",
    "\n",
    "        #We use np.argpartition, to only partially sort the top_k results\n",
    "        top_results = np.argpartition(-cos_scores, range(top_k))[0:top_k]\n",
    "\n",
    "        max_idx = top_results[0]\n",
    "        max_similarity = round(float(cos_scores[max_idx]), 4)\n",
    "        max_sentence = judgements[max_idx].strip()\n",
    "\n",
    "        #f = open('./Sentence-Embedding-Is-All-You-Need/data.csv', 'a', encoding='utf-8-sig', newline='')\n",
    "        #wr = csv.writer(f)\n",
    "        wr.writerow([str(case_number), document_id, judgement_id, collapse_whitespace(document), max_similarity, collapse_whitespace(max_sentence)])\n",
    "    \n",
    "        #if max_similarity != 0 and max_similarity != 1 and len(document) > 15 and len(max_sentence) > 15:\n",
    "        #    wr.writerow([collapse_whitespace(document), max_similarity, collapse_whitespace(max_sentence)])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./Sentence-Embedding-Is-All-You-Need/data_output_dec.pickle', 'rb') as datafile:\n",
    "    data = pickle.load(datafile)\n",
    "\n",
    "f = open('./Sentence-Embedding-Is-All-You-Need/data.csv', 'w', encoding='utf-8-sig', newline='')\n",
    "wr = csv.writer(f)\n",
    "wr.writerow(['case_number', 'brief_edms_id', 'judgement_edms_id', 'brief_sentence', 'similarity', 'similar_judgement_sentence'])\n",
    "\n",
    "prior_case_number = 0\n",
    "document_text_list = []\n",
    "judgement_text = ''\n",
    "\n",
    "for idx, row in tqdm(data.iterrows(), total=len(list(data.iterrows()))):\n",
    "    case_number = row[1]\n",
    "    edms_id = row[2]\n",
    "    document_type = row[4]\n",
    "    text = str(row[6])\n",
    "\n",
    "    try:\n",
    "        if case_number != prior_case_number:\n",
    "            if len(document_text_list) and len(judgement_text):\n",
    "                for document_text in document_text_list:\n",
    "                    calculate_similarity(case_number, document_text, judgement_text, wr)\n",
    "\n",
    "            prior_case_number = case_number\n",
    "            document_text_list = []\n",
    "            judgement_text = ''\n",
    "\n",
    "        # 01-판결문, 011-답변서, 010-준비서면, 701-소장\n",
    "        if document_type == '01':\n",
    "            judgement_text = (edms_id, text)\n",
    "        else:\n",
    "            document_text_list.append((edms_id, text))\n",
    "    \n",
    "    except:\n",
    "        print(document_type, text)\n",
    "        prior_case_number = case_number\n",
    "        document_text_list = []\n",
    "        judgement_text = ''\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./Sentence-Embedding-Is-All-You-Need/data_output_dec.pickle', 'rb') as datafile:\n",
    "    data = pickle.load(datafile)\n",
    "\n",
    "print(data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
