{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 데이터 전처리용 코드\n",
    "1. 추출형 요약용 데이터: tf-idf나 SentenceBERT를 통해서 얻은 문장 유사도 데이터 파일을 기반으로 KoBERTSum 등의 요약 모델에서 사용 가능한 extractive summarization용 데이터 파일을 생성합니다.\n",
    "2. 추상형 요약용 데이터: 법원 원데이터를 기반으로 준비서면/소장 텍스트를 전문으로, 판결문 텍스트를 요약문으로 하는 abstractive summarization용 데이터 파일을 생성합니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import csv\n",
    "import json\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "_whitespace = re.compile(r'\\s+')\n",
    "_valid_chars = re.compile(r\"[^ㄱ-ㅣ가-힣a-zA-Z0-9\\s;:,.!?¡¿—-…«»'\\\"“”~(){}[\\]\\-]\")  # 남겨둘 한글, 영문, 주요 특수문자 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collapse_whitespace(text):  # 중복된 공백 하나로 압축\n",
    "    return re.sub(_whitespace, ' ', text)\n",
    "\n",
    "\n",
    "def cleaner(text):  # 한글 + 영문 + 주요 특수문자 + 공백만 남기기\n",
    "    clean_text = re.sub(_valid_chars, '', text)\n",
    "    return clean_text\n",
    "\n",
    "\n",
    "def remover(text):  # 이상하거나 부자연스러운 표기 교정\n",
    "    text = text.replace('(idnum)', '')\n",
    "    text = text.replace('(phonenum)', '')\n",
    "    text = text.replace('idnum', '')\n",
    "    text = text.replace('phonenum', '')\n",
    "    text = text.replace('지 급 명 령 신 청 서', '지급명령신청서')\n",
    "    text = text.replace('청 구 취 지', '청구취지')\n",
    "    text = text.replace('준 비 서 면', '준비서면')\n",
    "    text = text.replace('참 고 서 면', '참고서면')\n",
    "    text = text.replace('입 증 방 법', '입증방법')\n",
    "    text = text.replace('참 고 자 료', '참고자료')\n",
    "    text = text.replace('답 변 서', '답변서')\n",
    "    text = text.replace('다 음 ', '다음 ')\n",
    "    text = text.replace('결 론', '결론')\n",
    "    text = text.replace(' 원 고 ', ' 원고 ')\n",
    "    text = text.replace(' 피 고 ', ' 피고 ')\n",
    "    text = text.replace(' 사 건 ', '사건')\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "brief_lens = list()\n",
    "judgement_lens = list()\n",
    "\n",
    "with open('./Sentence-Embedding-Is-All-You-Need/data.csv', 'r') as f:\n",
    "    reader = csv.reader(f)\n",
    "\n",
    "    for row in tqdm(reader):\n",
    "        brief_lens.append(len(row[3]))\n",
    "        judgement_lens.append(len(row[5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(brief_lens, bins=100, label='brief')\n",
    "plt.hist(judgement_lens, bins=100, label='judgement')\n",
    "plt.legend(loc='upper right')\n",
    "plt.axis([0, 2000, 0, 80000])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to Ext data\n",
    "# 매커니즘: 준비서면 문장별로 |를 경계로 묶고, 판결문과 유사한 내용을 담은 문장을 요약문에 포함시킴. 요약문에 포함된 문장 인덱스를 모아서 target으로 사용함.\n",
    "# 조건1: 준비서면쪽 문장 길이 제한 - 하한 상한\n",
    "# 조건2: 판결문쪽 문장 길이 - 하한 상한\n",
    "# 조건3: 유사도 제한 - 하한 상한\n",
    "# 결과물: ext_data.csv 확인\n",
    "\n",
    "# hyper-parameter tuning 필요함\n",
    "min_brief_len = 15\n",
    "max_brief_len = 500\n",
    "\n",
    "min_judgement_len = 15\n",
    "max_judgement_len = 750\n",
    "\n",
    "min_similarity = 0.75\n",
    "max_similarity = 0.95\n",
    "\n",
    "data_list = list()\n",
    "\n",
    "with open('./Sentence-Embedding-Is-All-You-Need/data.csv', 'r', encoding='utf-8-sig', newline='') as f:\n",
    "    reader = csv.reader(f)\n",
    "\n",
    "    current_case = None\n",
    "    current_document = None\n",
    "    target_judgement = None\n",
    "    sentences = list()\n",
    "    target_index_list = list()\n",
    "    sentence_idx = 0\n",
    "\n",
    "    for idx, row in tqdm(enumerate(reader), total=82192):\n",
    "        if idx != 0:\n",
    "            if current_document != row[1]:\n",
    "                if len(target_index_list):\n",
    "                    summary = ''\n",
    "                    for i in target_index_list:\n",
    "                        summary += sentences[i]\n",
    "                    data_list.append([current_case, current_document, target_judgement, '|'.join(sentences), target_index_list, summary])\n",
    "\n",
    "                current_case =  row[0] \n",
    "                current_document = row[1]\n",
    "                target_judgement = row[2]\n",
    "                sentences = list()\n",
    "                target_index_list = list()\n",
    "                sentence_idx = 0\n",
    "\n",
    "            brief_len = len(row[3])\n",
    "            judgement_len = len(row[5])\n",
    "            similarity = float(row[4])\n",
    "\n",
    "            # 주요 문장 필터링\n",
    "            if brief_len > min_brief_len and brief_len < max_brief_len and judgement_len > min_judgement_len and judgement_len < max_judgement_len and similarity > min_similarity and similarity < max_similarity:\n",
    "                target_index_list.append(sentence_idx)\n",
    "            \n",
    "            sentences.append(row[3])\n",
    "            sentence_idx += 1\n",
    "            \n",
    "    if len(target_index_list):\n",
    "        summary = ''\n",
    "        for i in target_index_list:\n",
    "            summary += sentences[i]\n",
    "        data_list.append([current_case, current_document, target_judgement, '|'.join(sentences), target_index_list, summary])\n",
    "\n",
    "\n",
    "with open('./Sentence-Embedding-Is-All-You-Need/ext_data.csv', 'w', encoding='utf-8-sig', newline='') as f:\n",
    "    wr = csv.writer(f)\n",
    "    wr.writerow(['case_number', 'brief_edms_id', 'judgement_edms_id', 'brief_sentence', 'sentence_index', 'summary'])\n",
    "    \n",
    "    for data in tqdm(data_list, total=len(data_list)):\n",
    "        wr.writerow(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to Ext data (Dacon 데이터 형식)\n",
    "# 매커니즘: 준비서면 문장별로 모아 리스트를 만들고, 판결문과 유사한 내용을 담은 문장을 요약문으로 상정함. 요약문에 포함된 문장 인덱스를 모아서 target으로 사용함.\n",
    "# 조건1: 준비서면쪽 문장 길이 제한 - 하한 상한\n",
    "# 조건2: 판결문쪽 문장 길이 - 하한 상한\n",
    "# 조건3: 유사도 제한 - 하한 상한\n",
    "# 결과물: train.jsonl과 extractive_test_v2.jsonl 확인\n",
    "\n",
    "# hyper-parameter tuning 필요함\n",
    "min_brief_len = 15\n",
    "max_brief_len = 500\n",
    "\n",
    "min_judgement_len = 15\n",
    "max_judgement_len = 750\n",
    "\n",
    "min_similarity = 0.75\n",
    "max_similarity = 0.95\n",
    "\n",
    "data_list = list()\n",
    "\n",
    "with open('./Sentence-Embedding-Is-All-You-Need/data.csv', 'r', encoding='utf-8-sig', newline='') as f:\n",
    "    reader = csv.reader(f)\n",
    "\n",
    "    current_case = None\n",
    "    current_document = None\n",
    "    target_judgement = None\n",
    "    sentences = list()\n",
    "    target_index_list = list()\n",
    "    sentence_idx = 0\n",
    "\n",
    "    for idx, row in tqdm(enumerate(reader), total=82192):\n",
    "        if idx != 0:\n",
    "            if current_document != row[1]:\n",
    "                if len(target_index_list):\n",
    "                    summary = ''\n",
    "                    for i in target_index_list:\n",
    "                        summary += sentences[i]\n",
    "                    data_list.append([current_case, current_document, sentences, summary, target_index_list])\n",
    "\n",
    "                current_case =  row[0] \n",
    "                current_document = row[1]\n",
    "                target_judgement = row[2]\n",
    "                sentences = list()\n",
    "                target_index_list = list()\n",
    "                sentence_idx = 0\n",
    "\n",
    "            brief_len = len(row[3])\n",
    "            judgement_len = len(row[5])\n",
    "            similarity = float(row[4])\n",
    "            if brief_len > min_brief_len and brief_len < max_brief_len and judgement_len > min_judgement_len and judgement_len < max_judgement_len and similarity > min_similarity and similarity < max_similarity:\n",
    "                target_index_list.append(sentence_idx)\n",
    "            \n",
    "            sentences.append(row[3])\n",
    "            sentence_idx += 1\n",
    "            \n",
    "    if len(target_index_list):\n",
    "        summary = ''\n",
    "        for i in target_index_list:\n",
    "            summary += sentences[i]\n",
    "        data_list.append([current_case, current_document, sentences, summary, target_index_list])\n",
    "\n",
    "with open(\"./Sentence-Embedding-Is-All-You-Need/train.jsonl\" , encoding= \"utf-8-sig\",mode=\"w\") as f: \n",
    "    for data in tqdm(data_list[:2400], total=len(data_list[:2400])):\n",
    "        line = {\"media\":data[0],\"id\":data[1],\"article_original\":data[2],\"abstractive\":data[3],\"extractive\":data[4]}\n",
    "        f.write(json.dumps(line) + \"\\n\")\n",
    "        \n",
    "with open(\"./Sentence-Embedding-Is-All-You-Need/extractive_test_v2.jsonl\" , encoding= \"utf-8-sig\",mode=\"w\") as f: \n",
    "    for data in tqdm(data_list[2400:], total=len(data_list[2400:])):\n",
    "        line = {\"media\":data[0],\"id\":data[1],\"article_original\":data[2],\"abstractive\":data[3],\"extractive\":data[4]}\n",
    "        f.write(json.dumps(line) + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pandas를 통해 jsonl의 데이터 확인\n",
    "with open('./Sentence-Embedding-Is-All-You-Need/ext_data_test.jsonl', 'r', encoding='utf-8-sig') as json_file:\n",
    "    json_list = list(json_file)\n",
    "\n",
    "trains = []\n",
    "for json_str in json_list:\n",
    "    line = json.loads(json_str)\n",
    "    trains.append(line)\n",
    "\n",
    "train_df = pd.DataFrame(trains[0])\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to Abs data\n",
    "# 매커니즘: 준비서면/소장-판결문이 매칭된 데이터셋 제작.\n",
    "\n",
    "data_list = list()\n",
    "\n",
    "with open('./Sentence-Embedding-Is-All-You-Need/data_output_dec.pickle', 'rb') as datafile:\n",
    "    data = pickle.load(datafile)\n",
    "\n",
    "    current_case_number = None\n",
    "    brief_id_list = list()\n",
    "    brief_text_list = list()\n",
    "    judgement_text = ''\n",
    "\n",
    "    for idx, row in tqdm(data.iterrows(), total=len(list(data.iterrows()))):\n",
    "        case_number = row[1]\n",
    "        edms_id = row[2]\n",
    "        document_type = row[4]\n",
    "        text = str(row[6])\n",
    "\n",
    "        try:\n",
    "            if case_number != current_case_number:\n",
    "                if len(brief_text_list) and len(judgement_text):\n",
    "                    for brief_id, brief_text in zip(brief_id_list, brief_text_list):\n",
    "                        data_list.append([current_case_number, brief_id, judgement_id, collapse_whitespace(cleaner(remover(brief_text))), collapse_whitespace(cleaner(remover(judgement_text)))])\n",
    "                \n",
    "                current_case_number = case_number\n",
    "                brief_id_list = list()\n",
    "                brief_text_list = list()\n",
    "                judgement_text = ''\n",
    "\n",
    "            # 01-판결문, 011-답변서, 010-준비서면, 701-소장\n",
    "            if document_type == '01':\n",
    "                judgement_id = edms_id\n",
    "                judgement_text = text\n",
    "            \n",
    "            else:\n",
    "                brief_id_list.append(edms_id)\n",
    "                brief_text_list.append(text)\n",
    "        \n",
    "        except:\n",
    "            print(document_type, text)\n",
    "            current_case_number = case_number\n",
    "            brief_id_list = list()\n",
    "            brief_text_list = list()\n",
    "            judgement_text = ''\n",
    "            pass\n",
    "\n",
    "\n",
    "with open('./Sentence-Embedding-Is-All-You-Need/abs_data.csv', 'w', encoding='utf-8-sig', newline='') as f:\n",
    "    wr = csv.writer(f)\n",
    "\n",
    "    wr.writerow(['case_number', 'brief_edms_id', 'judgement_edms_id', 'brief_text', 'judgement_text'])\n",
    "    \n",
    "    for data in tqdm(data_list, total=len(data_list)):\n",
    "        wr.writerow(data)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
